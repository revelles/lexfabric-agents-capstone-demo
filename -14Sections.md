# LexFabric Agents ‚Äì Capstone Demo (Google AI Agents Intensive 2025)

**Track:** Freestyle  
**Project Type:** Multi-Agent Evidence & Timeline Reasoning System  
**Author:** Francisco Revelles  
**Repository:** `lexfabric-agents-capstone-demo`

---

## üöÄ Overview

This project is a **multi-agent litigation analysis system** built for the 5-Day AI Agents Intensive by Google (Nov 10‚Äì14, 2025).  
It demonstrates how agents can:

- ingest structured/unstructured evidence
- index and hash files for chain-of-custody
- build chronological timelines from scattered records
- answer natural-language questions about a case  
- expose a simple CLI interface for querying

All data in this repository is **synthetic**, generated by the Synthetic Evidence Server.

---

## üß± Architecture

### **Agents**
- **Ingest Agent** ‚Äì loads evidence and metadata
- **Analysis Agent** ‚Äì normalizes files, extracts entities, builds summaries
- **Timeline Agent** ‚Äì turns events into chronological ordered records
- **Q&A Agent** ‚Äì answers questions by searching the memory bank and timeline
- **Controller** ‚Äì orchestrates tool calls and message routing

### **MCP Server (Synthetic Evidence Server)**  
Implements:
- `list_evidence(category, max_items)`
- `get_evidence_text(path)`
- `list_cases`
- `select_case(case_id)`
- `get_evidence_hashes()`

---

## üìÅ Repository Layout
capstone/
agents/
data_pipeline/
synthetic_evidence/
CC02/
RH10/
integration/
interfaces/cli.py
docs/
kaggle_writeup.md
video_script.md
README.md
setup.sh


---

## üíª Command-Line Usage

### **List evidence**
```bash
python -m capstone.demo --mode list-evidence --case-id CC02

List timeline items
python -m capstone.demo --mode list-timeline --case-id CC02

Ask a question
python -m capstone.demo --mode ask --case-id CC02 --query "What happened after the email on Feb 4?"

üîê Evidence Integrity

All evidence files produce a SHA-256 manifest (via the hashing tool).
This maintains:

reproducible agent behavior

integrity checks across sessions

support for chain-of-custody

Run:

python -m capstone.data_pipeline.hash_all

üì¶ Installation
git clone https://github.com/revelles/lexfabric-agents-capstone-demo.git
cd lexfabric-agents-capstone-demo
chmod +x setup.sh
./setup.sh

üß™ Requirements

Python 3.10+

Virtual environment recommended

üìÑ License

MIT License.

üèÅ Status

This version is the Capstone submission build, with:

multi-agent pipeline complete

MCP Synthetic Evidence Server integrated

CLI demo included

sample cases CC02 ‚Üí RH10

Future work:

LexFabric private version

integration with Alfresco + Notion


---

# ‚úÖ **2. docs/kaggle_writeup.md (1500-word submission)**

```markdown
# LexFabric Agents ‚Äì Capstone Writeup  
Google AI Agents Intensive (Nov 2025)

## 1. Problem & Motivation

Legal workflows involve enormous volumes of timelines, filings, emails, and documents that must be reconciled to understand ‚Äúwhat actually happened.‚Äù Human practitioners must manually re-read documents, reconstruct sequences, and cross-check records.

The goal of this project was to build a **multi-agent system** capable of:

- reading synthetic evidence files  
- extracting structured records  
- generating timelines  
- answering natural-language questions  
- maintaining reproducible integrity via hashing  

This is a miniature demonstration of the larger LexFabric MDLS (Multi-Docket Litigation System).  
The capstone version intentionally uses synthetic data only.

---

## 2. System Overview

The system is composed of **three major parts**:

1. **Synthetic Evidence Server (MCP)**  
   Provides evidence listing, text extraction, and hashing.

2. **Multi-Agent Pipeline**  
   Ingest ‚Üí Analysis ‚Üí Timeline ‚Üí Q&A.

3. **CLI Interface**  
   A simple frontend where a user can browse evidence and ask questions.

This division mirrors real-world legal-tech environments where evidence ingestion, chronological reconstruction, and attorney-facing interfaces are separate concerns.

---

## 3. Multi-Agent Pipeline

### 3.1 Ingest Agent
Responsible for:
- reading evidence metadata
- pulling file contents from the server
- categorizing files (emails, pleadings, notes, timeline fragments)

### 3.2 Analysis Agent
Uses LLM tools to:
- extract dates
- identify actors
- detect key entities
- normalize prose

This output is stored in a local ‚Äúmemory bank‚Äù for downstream agents.

### 3.3 Timeline Agent
Receives structured fragments and merges them into an ordered list, stored as a timeline dataset.

### 3.4 Q&A Agent
Queries both:
- the memory bank (semantic chunks)
- the timeline (ordered events)

It combines both for precise answers, such as:

> ‚ÄúWhat occurred after the first filing?‚Äù  
> ‚ÄúWho sent the email regarding the complaint?‚Äù

---

## 4. MCP Server Design

The server offers a minimal toolset:
- `list_evidence`
- `get_evidence_text`
- `list_cases`
- `select_case(case_id)`
- `get_evidence_hashes`

This allows the LLM agents to read and process only what they need.

---

## 5. Evidence Hashing

A SHA-256 manifest ensures integrity.

Pipeline:


walk ‚Üí load ‚Üí hash ‚Üí record ‚Üí save csv/json


Agents can later check whether any evidence changed and re-ingest only modified files.

---

## 6. CLI Interface

The CLI keeps the demo fully **offline-capable**.

Examples:

### List evidence


python -m capstone.demo --mode list-evidence --case-id CC02


### Ask:


python -m capstone.demo --mode ask --query "Summarize the filing sequence"


---

## 7. Case Data ‚Äì Synthetic Only

Ten synthetic cases were produced (CC02‚ÄìRH10).  
Each contains:
- 2‚Äì10 evidence records  
- at least one timeline fragment  
- at least one email or filing

This was intentionally designed to show wide generalization.

---

## 8. Development Strategy

The project was built in phases:

1. **Evidence server working standalone**  
2. **Ingest agent + extraction**  
3. **Timeline agent merging algorithm**  
4. **Q&A agent with hybrid search**  
5. **Full CLI pipeline**  
6. **Case selector + hashing tool**  

The capstone submission includes all components.

---

## 9. Observability

Each agent logs:
- confidence levels  
- reasoning summaries  
- tools invoked  
- tokens used  
- timeline diffs when merging  

---

## 10. Results

The system successfully:
- extracted dates reliably  
- built timelines even from fragmented records  
- answered event-sequence questions accurately  
- handled multiple synthetic cases  
- demonstrated reproducibility of pipeline via hashing  

---

## 11. Challenges

- Synthetic cases varied greatly in density  
- Timeline merging had to handle mismatched partial information  
- Ensuring deterministic agent behavior required hashing + caching  

---

## 12. Lessons Learned

- Agents benefit from specialized roles rather than monolithic prompts  
- Tool use (MCP) dramatically increases precision  
- Reproducibility is crucial for legal AI systems  
- Offline CLI demos are stable and predictable for evaluation  

---

## 13. Future Work

- Full LexFabric MDLS integration  
- Automated cross-case conflict detection  
- Alfresco + Notion connectors  
- Server-mode reasoning with multiple agents and long-running sessions  

---

## 14. Conclusion

This capstone demonstrates a practical, working example of a multi-agent evidence analysis system applicable to legal workflows, but generalizable to any structured + unstructured data domain.

It meets the AI Agents Intensive goals by:
- using tool-enabled agents  
- orchestrating multiple roles  
- integrating evidence  
- producing actionable outputs  

The version included in this repository is the public synthetic variant.  
A private version (LexFabric MDLS) will build upon the same foundation with secure evidence ingestion and multi-docket reasoning.

